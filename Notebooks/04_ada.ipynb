{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33d2ae77"
      },
      "source": [
        "# Task\n",
        "Split the provided notebook cell into multiple cells, adding code explanations and refining the code to be more professional. The goal is to improve the readability and structure of the notebook while maintaining the functionality of the original code, which involves training an AdaBoost model for classification and generating a submission file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13fed351"
      },
      "source": [
        "## Split and refine imports and data loading\n",
        "\n",
        "### Subtask:\n",
        "Create separate cells for imports, mounting Google Drive, defining file paths, and loading data with explanations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c65c95f"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the first cell for all the necessary imports with explanations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50fcfb57"
      },
      "source": [
        "# -----------------------------\n",
        "# 1️⃣ Imports\n",
        "# -----------------------------\n",
        "# pandas is used for data manipulation and analysis.\n",
        "import pandas as pd\n",
        "\n",
        "# AdaBoostClassifier is the boosting algorithm.\n",
        "# DecisionTreeClassifier is used as the base estimator for AdaBoost.\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# train_test_split is used to split the data into training and validation sets.\n",
        "# RandomizedSearchCV is used for hyperparameter tuning of the model.\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "# accuracy_score and classification_report are used to evaluate the model's performance.\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# drive is used to mount Google Drive to access files stored there.\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8caaab9"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new cell to mount Google Drive and add a comment explaining the purpose.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "357b8b93",
        "outputId": "c392ec90-6f3f-4e89-81e5-82662c87ccda"
      },
      "source": [
        "# -----------------------------\n",
        "# 2️⃣ Mount Google Drive\n",
        "# -----------------------------\n",
        "# Mount Google Drive to access files stored in 'MyDrive'.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b57255fa"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new cell to define the file paths for the datasets and add comments explaining each path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3179173"
      },
      "source": [
        "# -----------------------------\n",
        "# 3️⃣ Define File Paths\n",
        "# -----------------------------\n",
        "# Define file paths based on the project structure within Google Drive.\n",
        "train_processed_path = '/content/drive/MyDrive/ml_project/data/train_preprocessed.csv'\n",
        "test_processed_path = '/content/drive/MyDrive/ml_project/data/test_preprocessed.csv'\n",
        "sample_submission_path = '/content/drive/MyDrive/ml_project/data/sample_submission.csv'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fad2db4"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the final cell for loading the datasets into pandas DataFrames and include a quick check of their shapes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3789d031",
        "outputId": "070d4717-326f-4422-e276-09395f16d5c1"
      },
      "source": [
        "# -----------------------------\n",
        "# 4️⃣ Load Data\n",
        "# -----------------------------\n",
        "# Load the preprocessed training and test datasets, and the sample submission file into pandas DataFrames.\n",
        "train_df = pd.read_csv(train_processed_path)\n",
        "test_df = pd.read_csv(test_processed_path)\n",
        "sample_submission_df = pd.read_csv(sample_submission_path)\n",
        "\n",
        "# Optional: quick check to display the shapes of the loaded dataframes.\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)\n",
        "print(\"Sample submission shape:\", sample_submission_df.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (15533, 22)\n",
            "Test shape: (5225, 21)\n",
            "Sample submission shape: (13840, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dd76d7a"
      },
      "source": [
        "## Explain feature and target preparation\n",
        "\n",
        "### Subtask:\n",
        "Create a cell with code and explanations for preparing the features (X) and target (y) variables, including the train-validation split.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3727fdf1"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a cell to prepare features and target, and split the data into training and validation sets with explanations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1d2d5da",
        "outputId": "cf636db5-565d-434c-879b-bc6c3dfebec2"
      },
      "source": [
        "# -----------------------------\n",
        "# 5️⃣ Prepare Features, Target, and Split Data\n",
        "# -----------------------------\n",
        "# Separate the features (X) from the target variable (y).\n",
        "# 'WeightCategory' is the target variable we want to predict.\n",
        "X = train_df.drop('WeightCategory', axis=1)\n",
        "y = train_df['WeightCategory']\n",
        "\n",
        "# Split the data into training and validation sets.\n",
        "# X_train, y_train: Data used for training the model.\n",
        "# X_val, y_val: Data used for evaluating the model's performance during development.\n",
        "# test_size=0.2: Allocate 20% of the data for validation.\n",
        "# random_state=42: Ensure the split is reproducible.\n",
        "# stratify=y: Maintain the same proportion of target classes in both train and validation sets.\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_val shape:\", y_val.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (12426, 21)\n",
            "X_val shape: (3107, 21)\n",
            "y_train shape: (12426,)\n",
            "y_val shape: (3107,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14dd4e4c"
      },
      "source": [
        "## Detail adaboost model definition and parameter grid\n",
        "\n",
        "### Subtask:\n",
        "Create a cell to define the AdaBoost model and explain the parameter grid used for hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6733a04"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a cell to define the AdaBoost model and its parameter grid with explanations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ca3d0ec",
        "outputId": "463759f3-6a96-4868-bc21-6a782129028b"
      },
      "source": [
        "# -----------------------------\n",
        "# 6️⃣ Define AdaBoost Model and Parameter Grid\n",
        "# -----------------------------\n",
        "\n",
        "# Define the base estimator for the AdaBoost model.\n",
        "# A Decision Tree Classifier is commonly used as a weak learner.\n",
        "# random_state=42 ensures reproducibility of the base estimator.\n",
        "base_estimator = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the AdaBoost Classifier model.\n",
        "# It boosts the performance of the base estimator.\n",
        "# base_estimator: The weak learner to use.\n",
        "# random_state=42 ensures reproducibility of the AdaBoost model.\n",
        "ada_model = AdaBoostClassifier(\n",
        "    estimator=base_estimator, # Corrected parameter name from base_estimator to estimator for newer versions of scikit-learn\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning using RandomizedSearchCV.\n",
        "param_grid = {\n",
        "    # n_estimators: The number of boosting stages to perform. Higher numbers can improve performance but increase training time.\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    # learning_rate: Weights the contribution of each weak learner. A lower learning rate requires more estimators.\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    # base_estimator__max_depth: The maximum depth of the decision tree base estimators. Shallow trees (small max_depth) are typical for boosting.\n",
        "    'estimator__max_depth': [1, 2, 3, 4] # Corrected parameter name from base_estimator__max_depth to estimator__max_depth for newer versions of scikit-learn\n",
        "}\n",
        "\n",
        "print(\"AdaBoost model defined.\")\n",
        "print(\"Parameter grid defined for tuning:\")\n",
        "print(param_grid)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost model defined.\n",
            "Parameter grid defined for tuning:\n",
            "{'n_estimators': [50, 100, 200, 300], 'learning_rate': [0.01, 0.05, 0.1, 0.2], 'estimator__max_depth': [1, 2, 3, 4]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f1d9e60"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a cell for RandomizedSearchCV to find the best hyperparameters for the AdaBoost model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2445df6",
        "outputId": "adf50af7-29a7-4895-9dca-4c1082c82474"
      },
      "source": [
        "# -----------------------------\n",
        "# 7️⃣ Perform Randomized Search for Hyperparameter Tuning\n",
        "# -----------------------------\n",
        "\n",
        "# Initialize RandomizedSearchCV to find the best hyperparameters.\n",
        "# estimator: The model to tune (AdaBoostClassifier).\n",
        "# param_distributions: The grid of hyperparameters to sample from.\n",
        "# n_iter: Number of parameter settings that are sampled. Higher is better but slower.\n",
        "# cv: Number of folds for cross-validation.\n",
        "# scoring: Metric to evaluate the model performance (accuracy).\n",
        "# n_jobs=-1: Use all available CPU cores for faster processing.\n",
        "# verbose=1: Display progress during the search.\n",
        "# random_state=42: Ensure reproducibility of the random sampling.\n",
        "rand_search_ada = RandomizedSearchCV(\n",
        "    estimator=ada_model,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=30,        # 30 random combinations → fast enough for demonstration\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV to the training data to find the best hyperparameters.\n",
        "rand_search_ada.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found and the corresponding best cross-validation score.\n",
        "print(\"\\nBest hyperparameters found by RandomizedSearchCV:\", rand_search_ada.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", rand_search_ada.best_score_)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
            "\n",
            "Best hyperparameters found by RandomizedSearchCV: {'n_estimators': 200, 'learning_rate': 0.1, 'estimator__max_depth': 4}\n",
            "Best cross-validation accuracy: 0.883148237566393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f6964c6"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a cell to evaluate the best AdaBoost model on the validation set and display the performance metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4d7896d",
        "outputId": "09da2cef-9360-492a-bf3b-ae9400b2c731"
      },
      "source": [
        "# -----------------------------\n",
        "# 8️⃣ Evaluate the Best Model on the Validation Set\n",
        "# -----------------------------\n",
        "\n",
        "# Get the best estimator found by RandomizedSearchCV.\n",
        "best_ada = rand_search_ada.best_estimator_\n",
        "\n",
        "# Make predictions on the validation set using the best model.\n",
        "y_val_pred = best_ada.predict(X_val)\n",
        "\n",
        "# Evaluate the model's performance on the validation set.\n",
        "# Calculate and print the accuracy score.\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
        "\n",
        "# Print the classification report, which includes precision, recall, F1-score, and support for each class.\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_val_pred))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8818796266495011\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.92       374\n",
            "           1       0.87      0.85      0.86       469\n",
            "           2       0.86      0.83      0.84       441\n",
            "           3       0.94      0.97      0.95       481\n",
            "           4       0.99      1.00      0.99       597\n",
            "           5       0.74      0.75      0.74       369\n",
            "           6       0.78      0.76      0.77       376\n",
            "\n",
            "    accuracy                           0.88      3107\n",
            "   macro avg       0.87      0.87      0.87      3107\n",
            "weighted avg       0.88      0.88      0.88      3107\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a4f7df2"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a cell to retrain the best AdaBoost model on the full training dataset and then predict on the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaf7d111"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates that the `AdaBoostClassifier` constructor received an unexpected keyword argument 'estimator__max_depth'. This is because the hyperparameter 'estimator__max_depth' belongs to the base estimator (DecisionTreeClassifier), not the AdaBoostClassifier itself. The parameters should be passed correctly to the respective estimators. I need to correct how the best parameters are passed to the `AdaBoostClassifier` and its `base_estimator`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJssB6n4BWRa",
        "outputId": "c425c2cb-90e7-4f3a-df9f-7a049af4b10c"
      },
      "source": [
        "# -----------------------------\n",
        "# 9️⃣ Retrain on Full Training Data and Predict on Test Set - Fixed Again\n",
        "# -----------------------------\n",
        "\n",
        "# Get the best parameters from the randomized search result.\n",
        "best_params = rand_search_ada.best_params_\n",
        "\n",
        "# Extract the base estimator parameters (like max_depth) and AdaBoost parameters (like n_estimators, learning_rate).\n",
        "base_estimator_params = {key.split('__')[1]: value for key, value in best_params.items() if '__' in key}\n",
        "ada_params = {key: value for key, value in best_params.items() if '__' not in key}\n",
        "\n",
        "# Define the base estimator with its best parameters.\n",
        "base_estimator_full = DecisionTreeClassifier(random_state=42, **base_estimator_params)\n",
        "\n",
        "# Retrain the AdaBoost model using the optimal hyperparameters found by RandomizedSearchCV\n",
        "# on the entire training dataset.\n",
        "# Drop the 'id' column from the full training data before retraining.\n",
        "X_full_train = X.drop('id', axis=1)\n",
        "\n",
        "best_ada_full = AdaBoostClassifier(\n",
        "    estimator=base_estimator_full, # Pass the configured base estimator\n",
        "    random_state=42,              # Ensure reproducibility\n",
        "    **ada_params                  # Unpack the AdaBoost specific parameters\n",
        ")\n",
        "best_ada_full.fit(X_full_train, y)\n",
        "\n",
        "# Make predictions on the preprocessed test dataset (test_df) using the retrained model.\n",
        "# Drop the 'id' column from the test_df before prediction.\n",
        "X_test = test_df.drop('id', axis=1)\n",
        "y_test_pred = best_ada_full.predict(X_test)\n",
        "\n",
        "print(\"Model retrained on full training data (excluding 'id').\")\n",
        "print(\"Predictions made on the test set (excluding 'id').\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model retrained on full training data (excluding 'id').\n",
            "Predictions made on the test set (excluding 'id').\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff7262cf"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps have trained the model and generated predictions. The next step is to map the numeric predictions back to the original string labels and then prepare the submission file in the required format for Kaggle.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e973eab1",
        "outputId": "689c06fe-0426-4cf0-e9c7-8e2a221f5765"
      },
      "source": [
        "# -----------------------------\n",
        "# 10️⃣ Map Numeric Predictions to String Labels\n",
        "# -----------------------------\n",
        "\n",
        "# Define the mapping from the numeric class labels (as used by scikit-learn)\n",
        "# back to the original string labels representing the weight categories.\n",
        "label_map = {\n",
        "    0: 'Insufficient_Weight',\n",
        "    1: 'Normal_Weight',\n",
        "    2: 'Obesity_Type_I',\n",
        "    3: 'Obesity_Type_II',\n",
        "    4: 'Obesity_Type_III',\n",
        "    5: 'Overweight_Level_I',\n",
        "    6: 'Overweight_Level_II'\n",
        "}\n",
        "\n",
        "# Map the numeric predictions from the test set (y_test_pred) to their corresponding string labels\n",
        "# using a list comprehension and the defined label_map.\n",
        "y_test_labels = [label_map[num] for num in y_test_pred]\n",
        "\n",
        "print(\"Numeric predictions mapped to string labels.\")\n",
        "\n",
        "# -----------------------------\n",
        "# 11️⃣ Prepare Kaggle Submission File\n",
        "# -----------------------------\n",
        "\n",
        "# Create a pandas DataFrame for the submission file.\n",
        "# It should contain two columns: 'id' and 'WeightCategory'.\n",
        "# The 'id' column is taken from the original test_df (which still contains the ids).\n",
        "# The 'WeightCategory' column contains the predicted string labels.\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_df['id'],  # Use the 'id' column from the original test_df\n",
        "    'WeightCategory': y_test_labels\n",
        "})\n",
        "\n",
        "# Define the path where the submission file will be saved in Google Drive.\n",
        "submission_path = '/content/drive/MyDrive/ml_project/data/ada_submission.csv'\n",
        "\n",
        "# Save the submission DataFrame to a CSV file.\n",
        "# index=False prevents pandas from writing the DataFrame index as a column in the CSV.\n",
        "submission.to_csv(submission_path, index=False)\n",
        "\n",
        "print(f\"AdaBoost submission saved successfully to: {submission_path}\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeric predictions mapped to string labels.\n",
            "AdaBoost submission saved successfully to: /content/drive/MyDrive/ml_project/data/ada_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pD5X5rCLB-Qt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}